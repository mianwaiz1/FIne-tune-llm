{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e460bc0-ae50-4bb9-8186-8bc6101c8a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is for evaluating model\n",
    "# Load base vs fine-tuned models.\n",
    "# Generate predictions for a sample of your dataset.\n",
    "# Compare both using ROUGE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e7f994-0f19-4e97-b8c3-cd28c2e7e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.2.2 transformers==4.41.2 datasets==2.20.0 evaluate==0.4.2 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3c34d6",
   "metadata": {},
   "source": [
    "The json module is used to handle dataset files.\n",
    "datasets.Dataset organizes the data into a Hugging Face-compatible format.\n",
    "AutoTokenizer and AutoModelForSeq2SeqLM load a pretrained sequence-to-sequence model along with its tokenizer.\n",
    "The evaluate library provides standardized metrics (like ROUGE and BLEU) to measure the modelâ€™s performance.\n",
    "Finally, torch enables tensor computations and GPU acceleration, which are essential for training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106187a8-899d-4552-9651-339de50c8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89dbbd",
   "metadata": {},
   "source": [
    "loading data set lung_cancer.jsonl and splitting it into train_test for evaluating our base model and fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d9947-2a9b-43aa-b9ca-3ae48c6753a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from JSONL\n",
    "def load_dataset(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "dataset = load_dataset(\"lung_cancer.jsonl\")   # <-- replace with your dataset\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# âœ… Use training data for evaluation\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be38e16",
   "metadata": {},
   "source": [
    "loading our base and fine tuned models and tokensizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad09587-daed-4e1c-9bae-dab9e65ea01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base (pre-trained) model\n",
    "base_model_name = \"google/flan-t5-small\"\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "\n",
    "# Fine-tuned model\n",
    "fine_tuned_path = \"./fine_tuned_lung_cancer\"\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(fine_tuned_path)\n",
    "ft_model = AutoModelForSeq2SeqLM.from_pretrained(fine_tuned_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7415b",
   "metadata": {},
   "source": [
    "generating answers by both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c0e7f-b9f2-4fe5-aa47-37dd65861e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating answers for evaluating\n",
    "def generate_answer(model, tokenizer, question):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f6c5e7",
   "metadata": {},
   "source": [
    "evaluating models using rouge metrics because it gives a better evaluation of model by comparing predicted answer and refrence answer \n",
    "rouge1 means word by word evaluating \n",
    "rouge2 means evaluating by short phrases \n",
    "rougeL means evaluating full sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca94597-9e9c-4492-a4f9-78a4160f22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(train_data, n_samples=50):\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    # Use a subset to keep it fast\n",
    "    subset = train_data.select(range(min(n_samples, len(train_data))))\n",
    "\n",
    "    base_preds, ft_preds, refs = [], [], []\n",
    "    for example in subset:\n",
    "        q = example[\"input\"]\n",
    "        ref = example[\"output\"]\n",
    "\n",
    "        base_pred = generate_answer(base_model, base_tokenizer, q)\n",
    "        ft_pred = generate_answer(ft_model, ft_tokenizer, q)\n",
    "\n",
    "        base_preds.append(base_pred)\n",
    "        ft_preds.append(ft_pred)\n",
    "        refs.append(ref)\n",
    "\n",
    "    print(\"\\nðŸ“Š Evaluation Results (ROUGE on TRAIN DATA):\")\n",
    "\n",
    "    base_scores = rouge.compute(predictions=base_preds, references=refs)\n",
    "    ft_scores = rouge.compute(predictions=ft_preds, references=refs)\n",
    "\n",
    "    print(\"\\n--- Base Model ---\")\n",
    "    for k, v in base_scores.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Fine-tuned Model ---\")\n",
    "    for k, v in ft_scores.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
